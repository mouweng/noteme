# 海量数据处理问题

- [教你如何迅速秒杀掉：99%的海量数据处理面试题](https://blog.csdn.net/WantFlyDaCheng/article/details/81531994)

## 模板

### 思路

1. 分治：hash取模
2. 处理：hash统计
3. 合并：堆排序/归并/快排

### 无序的数组求TopK

> 求TopK大就用最小堆；求TopK小就用最大堆；

以TopK大为例，每次和堆顶元素进行比较（堆顶就是堆的守门员）：

- 比堆顶大就替换堆顶，计算新的堆顶
- 比堆顶小就直接过滤

时间复杂度`O(N)`

### 有序的数组求TopK

> 求TopK大就用最大堆；求TopK小就用最小堆；

以TopK小为例，维护一个大小为K的最小堆，先将每个数组的第一个数入堆，每次将堆顶出堆，把这个堆顶所在数组的下一个数入堆，再重新维护堆，以此类推，重复K次这样的操作就能得到topK。

时间复杂度`O(K)`

## 例题

### 例题1

> 有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M，返回频数最高的100个词。

1. 分治：顺序读文件中，对于每个词x，取hash(x)%5000，然后按照该值存到5000个小文件（记为x0,x1,…x4999）中，每个文件大概是200k左右。
2. 统计：对每个小文件，采用hashmap统计文件中出现的词以及相应的频率，存入文件。
3. 合并：求出每个文件频率最高的100个词（可以用最小堆）。再合并所有文件的词（将一组各自有序的数组求TopK，采用最大堆），求得频率最高的100个词。（也可以再用一次最小堆）

### 例题2

> 有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。

1. 分治：顺序读取10个文件，按照hash(query)%10的结果将query写入到另外10个文件（记为a0,a1,..a9）中。这样新生成的文件每个的大小大约也1G（假设hash函数是随机的）。
2. 统计：找一台内存在2G左右的机器，依次对用hash_map(query, query_count)来统计每个query出现的次数。
3. 合并：利用快速/堆/归并排序按照出现次数进行排序，将排序好的query和对应的query_cout输出到文件中，这样得到了10个排好序的文件。最后，对这10个文件进行归并排序（内排序与外排序相结合）。

### 例题3

> 给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？

hash映射（分文件） + hash_set统计（分文件逐个对比）

### 例题4

> 怎么在海量数据中找出重复次数最多的一个？

hash映射（分文件）+ hash_map统计 + 合并求最大

### 例题5

> 上千万或上亿数据（有重复），统计其中出现次数最多的前N个数据？

hash映射（分文件）+ hash_map统计 + 合并用堆求前N

### 例题6

> 1000万字符串，其中有些是重复的，需要把重复的全部去掉，保留没有重复的字符串。请怎么设计和实现？

### 例题7

> 500个数组，每个数组500个元素，每个数组从大到小拍好了序，求这500个数组中前500大的数

维护一个大小为500的最大堆，先将每个数组的第1个数入堆，然后最大值出堆，再进入此数所在数组的第二个数，一次类推，得到前500个最小的值。（相当于每一次将500个数组的最大值在堆中进行运算，选出最大的，重复500次）